{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains the step-by-step procedures adopted to consume the messages in Spark and run the transformations required to land the assessments data in the form and structure it needs to be to be queried by our clients.\n",
    "\n",
    "The document is organized in three chapters:\n",
    "- [Use Spark to transform the messages](#use_spark_to_transform_the_messages)\n",
    "- [Query the data using Spark SQL](#query_the_data_using_spark_sql)\n",
    "- [Land transformed data into HDFS](#land_transformed_data_into_HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='use_spark_to_transform_the_messages'></a>\n",
    "## 4. Use spark to transform the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Consume the messages from kafka into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "raw_assessments = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\", \"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cache to cut back on warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Print the schema in which data was imported from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Explore the *raw_assessments* data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way data was imported from kafka into spark. Each assessment (JSON entry) was imported as a row in the field value. The keys have null values, and the other fields: topic, partition, offset, timestamp, and timestampType are metadata that we don't need to keep in our queryable database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     5|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     6|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     7|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     8|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     9|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    10|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    11|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    12|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    13|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    14|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    15|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    16|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    17|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    18|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    19|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Select field *value* and cast it as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Explore the *assessments* data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can see clearly that each one of our assessment entries is represented as a line in the assessments dataframe. At this stage tough all information is compacted in a single field which makes this an unqueryable format. We will need to perform some transformations into the data before loading it into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Get first line of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessments.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Provide the data schema to spark to extract only fields of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a quite nested JSON file, with many levels, it will be hard for spark to get the implied schema from the data. So we will opt to provide the schema to spark (forced schema). We will take this opportunity also to select only the fields that should be more of interest of our clients to analyse.\n",
    "\n",
    "In this sense, we are keeping only the following fields:\n",
    "- **keen_id**: unique id to identify the assessment taken by a user, on a given exam, on a given timeframe\n",
    "- **base_exam_id**: id of the exam (tracking the exams is very important to understand which of our exams are the most popular or most difficult, for example)\n",
    "- **user_exam_id**: id of the user (tracking users is very important to understand user behaviour and enable personalization)\n",
    "- **exam_name**: name of the assessment\n",
    "- **certification**: certified vs. free assessments\n",
    "- **started_at**: timestamp of assessment (important to keep tracking of metrics over time or understand user behaviour in specific timeframes - ex. weekdays vs. weekends)\n",
    "- **max_attempts**: maximum number of attempts allowed\n",
    "- **unanswered**: number of questions unaswered\n",
    "- **incomplete**: number of questions not completed\n",
    "- **incorrect**: number of questions answered incorrecly\n",
    "- **correct**: number of questions answered correctly\n",
    "- **total**: total number of question in the exam\n",
    "\n",
    "We will not include on our final database other timestamps or ids that would be of lower interest. We will not include as well any of the details around questions and alternatives. Later on, if it were of interest of our clients to have access to that information we can retrieve the data from kafka again and save it in different tables with shared keys with our main asssesments table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "final_schema = StructType([StructField('keen_id', StringType(), True),\n",
    "                     StructField('base_exam_id', StringType(), True),\n",
    "                     StructField('user_exam_id', StringType(), True),\n",
    "                     StructField('exam_name', StringType(), True),\n",
    "                     StructField('certification', StringType(), True),\n",
    "                     StructField('started_at', StringType(), True),\n",
    "                     StructField('max_attempts', StringType(), True),\n",
    "                     StructField('sequences', StructType([\n",
    "                         StructField('counts', StructType([\n",
    "                             StructField('unanswered', IntegerType(), True),\n",
    "                             StructField('incomplete', IntegerType(), True),\n",
    "                             StructField('incorrect', IntegerType(), True),\n",
    "                             StructField('correct', IntegerType(), True),\n",
    "                             StructField('total', IntegerType(), True),\n",
    "                         ]))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Unwrap JSON mapping json.loads function into RDD (forced schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "focused_extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Check if JSON unwrapping worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+-------------+\n",
      "|             keen_id|        base_exam_id|        user_exam_id|           exam_name|certification|          started_at|max_attempts|    sequences|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+-------------+\n",
      "|5a6745820eb8ab000...|37f0a30a-7464-11e...|6d4089e4-bde5-4a2...|Normal Forms and ...|        false|2018-01-23T14:23:...|         1.0|[[0,1,1,2,4]]|\n",
      "|5a674541ab6b0a000...|37f0a30a-7464-11e...|2fec1534-b41f-441...|Normal Forms and ...|        false|2018-01-23T14:21:...|         1.0|[[0,2,1,1,4]]|\n",
      "|5a67999d3ed3e3000...|4beeac16-bb83-4d5...|8edbc8a8-4d26-429...|The Principles of...|        false|2018-01-23T20:22:...|         1.0|[[0,0,1,3,4]]|\n",
      "|5a6799694fc7c7000...|4beeac16-bb83-4d5...|c0ee680e-8892-4e6...|The Principles of...|        false|2018-01-23T20:21:...|         1.0|[[0,2,0,2,4]]|\n",
      "|5a6791e824fccd000...|6442707e-7488-11e...|e4525b79-7904-405...|Introduction to B...|        false|2018-01-23T19:48:...|         1.0|[[0,0,1,3,4]]|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "focused_extracted_assessments.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Check out the new dataframe schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* I kept the *max_attempts* field as string as my tentative to force it to integer did not work. I also kept the *started_at* field as string as my tentative to force it to timestamp did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- unanswered: integer (nullable = true)\n",
      " |    |    |-- incomplete: integer (nullable = true)\n",
      " |    |    |-- incorrect: integer (nullable = true)\n",
      " |    |    |-- correct: integer (nullable = true)\n",
      " |    |    |-- total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "focused_extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='query_the_data_using_spark_sql'></a>\n",
    "## 5. Query the data using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Transforming data into a queryable TempTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "focused_extracted_assessments.registerTempTable('focused_assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Using Spark SQL to query data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the report I will run some queries to showcase to our clients some of the questions they can answer using our data table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 How many assesstments are in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 3,280 assessments in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(keen_id)|\n",
      "+--------------+\n",
      "|          3280|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(keen_id) from focused_assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 What are the most popular assessments in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           exam_name|count|\n",
      "+--------------------+-----+\n",
      "|        Learning Git|  394|\n",
      "|Introduction to P...|  162|\n",
      "|Introduction to J...|  158|\n",
      "|Intermediate Pyth...|  158|\n",
      "|Learning to Progr...|  128|\n",
      "|Introduction to M...|  119|\n",
      "|Software Architec...|  109|\n",
      "|Beginning C# Prog...|   95|\n",
      "|    Learning Eclipse|   85|\n",
      "|Learning Apache M...|   80|\n",
      "|Beginning Program...|   79|\n",
      "|       Mastering Git|   77|\n",
      "|Introduction to B...|   75|\n",
      "|Advanced Machine ...|   67|\n",
      "|Learning Linux Sy...|   59|\n",
      "|JavaScript: The G...|   58|\n",
      "|        Learning SQL|   57|\n",
      "|Practical Java Pr...|   53|\n",
      "|    HTML5 The Basics|   52|\n",
      "|   Python Epiphanies|   51|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(keen_id) as count from focused_assessments group by exam_name order by count desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 What are the most difficult assessments (lower student performance)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           exam_name|pct_correct|\n",
      "+--------------------+-----------+\n",
      "|Example Exam For ...|       null|\n",
      "|Client-Side Data ...|        0.2|\n",
      "|       View Updating|       0.25|\n",
      "|Native Web Apps f...|       0.25|\n",
      "|Arduino Prototypi...|       0.33|\n",
      "|Mastering Advance...|       0.36|\n",
      "|           Nullology|       0.38|\n",
      "|Building Web Serv...|       0.42|\n",
      "| Mastering Web Views|       0.42|\n",
      "|Web & Native Work...|       0.42|\n",
      "|Cloud Computing W...|       0.43|\n",
      "|         Offline Web|       0.44|\n",
      "|Learning C# Best ...|       0.46|\n",
      "|Design Patterns i...|       0.47|\n",
      "|  Learning Java EE 7|       0.48|\n",
      "|Software Architec...|       0.48|\n",
      "|Data Visualizatio...|       0.49|\n",
      "|Being a Better In...|        0.5|\n",
      "|Hibernate and JPA...|        0.5|\n",
      "|Learning iPython ...|        0.5|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, round(sum(sequences.counts.correct)/sum(sequences.counts.total), 2) as pct_correct from focused_assessments group by exam_name order by pct_correct\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Is there any users taking more than one assessment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        user_exam_id|count|\n",
      "+--------------------+-----+\n",
      "|1e325cc1-47a9-480...|    3|\n",
      "|b7ac6d15-97e1-4e9...|    3|\n",
      "|a244c11a-d890-4e3...|    3|\n",
      "|3d63ec69-8d97-4f9...|    3|\n",
      "|028ad26f-a89f-4a6...|    3|\n",
      "|fa23b287-0d0a-468...|    3|\n",
      "|00745aef-f3af-412...|    3|\n",
      "|cdc5859d-b332-4fb...|    3|\n",
      "|37cf5b0c-4807-421...|    3|\n",
      "|ac80a11a-2e79-40e...|    3|\n",
      "|c320d47f-60d4-49a...|    3|\n",
      "|949aa36c-74c7-4fc...|    3|\n",
      "|d4ab4aeb-1368-486...|    3|\n",
      "|bd96cfbe-1532-4ba...|    3|\n",
      "|a7e6fc04-245f-4e3...|    3|\n",
      "|a45b5ee6-a4ed-4b1...|    3|\n",
      "|66d91177-c436-4ee...|    3|\n",
      "|6132da16-2c0c-436...|    3|\n",
      "|c1eb4d4a-d6ef-43e...|    2|\n",
      "|6e4889ab-5978-44b...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select user_exam_id, count(keen_id) as count from focused_assessments group by user_exam_id order by count desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='land_transformed_data_into_HDFS'></a>\n",
    "## 6. Land transformed data into HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Saving data in parquet format into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "focused_extracted_assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Check data in HDFS tmp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root   supergroup          0 2020-10-17 20:55 /tmp/assessments\n",
      "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2020-10-17 20:52 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
